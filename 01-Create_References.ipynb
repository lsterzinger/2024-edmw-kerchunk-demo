{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `kerchunk` and make sure it's at the latest version (`0.0.6` at the time of writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kerchunk\n",
    "kerchunk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If Kerchunk is not at the latest version, update with pip/conda: and **restart the kernel**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade kerchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import fsspec\n",
    "from glob import glob\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fsspec` -- What is it?\n",
    "* Provides unified interface to different filesystem types\n",
    "* Local, cloud, http, dropbox, Google Drive, etc\n",
    "    * All accessible with the same API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fsspec.registry import known_implementations\n",
    "known_implementations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open a new filesystem, of type `s3` (Amazon Web Services storage)\n",
    "This tells `fsspec` what type of storage system to use (AWS S3) and any authentication options (this is a public dataset, so use anonymous mode `anon=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `fs.glob()` to generate a list of files in a certain directory. Goes data is stored in `s3://noaa-goes16/<product>/<year>/<day_of_year>/<hour>/<datafile>.nc` format.\n",
    "\n",
    "This `glob()` returns all files in the 210th day of 2020 (July 28th, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist_full = fs.glob('s3://noaa-goes16/ABI-L1b-RadC/2024/050/*/*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{len(flist_full)} files. First 10:\")\n",
    "flist_full[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepend `s3://` to the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist = ['s3://' + f for f in flist_full[::20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening directly with xarray is slow, even in the same region:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you're in AWS, this will tell you your region name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "boto3.client('s3').meta.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below opens the first 10 files with `xr.open_mfdataset()` and takes ~50 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_files_to_open = 10\n",
    "ds = xr.open_mfdataset(\n",
    "    [\n",
    "        fs.open(f) for f in flist[:n_files_to_open]\n",
    "    ], \n",
    "    combine='nested',\n",
    "    concat_dim='t'\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of creating a single reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = flist[0]\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file object with `fsspec`, and process with Kerchunk. The `SingleHdf5ToZarr` class takes in the file object and its url as required arguments. The `inline_threshold` parameter sets the number of bytes a chunk must be to be stored directly in the metadata file (instead of a referenced byte-range). \n",
    "\n",
    "`.translate()` returns a `dict` of extracted metadata for use with `ReferenceFileSystem`. This can be aggregated with other metadata or written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reference = SingleHdf5ToZarr(u, inline_threshold=100, storage_options={'anon':True}).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('test.json', 'w') as outf:\n",
    "    outf.write(ujson.dumps(reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create references for all files in `flist`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With a Dask cluster\n",
    "[Dask](https://dask.org/) is a python package that allows for easily parallelizing python code. This section starts a local client (using whatever processors are available on the current machine). This can also be done just as easily using [Dask clusters](https://docs.dask.org/en/stable/deploying.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using ESIP Jupyterhub\n",
    "\n",
    "This should be used instead of the above local Dask cluster if running on ESIP's JupyterHub at https://nebari.esipfed.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, '/shared/users/lib/')\n",
    "\n",
    "from nebari_tools import start_dask_cluster, stop_dask_cluster\n",
    "\n",
    "client,cluster = start_dask_cluster(region='us-east-1', worker_min=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definte function to return a reference dictionary for a given S3 file URL\n",
    "\n",
    "This function does the following:\n",
    "1. `so` is a dictionary of options for `fsspec.open()`\n",
    "2. Use `fsspec.open()` to open the file given by URL `f`\n",
    "3. Using `kerchunk.SingleHdf5ToZarr()` and supplying the file object `infile` and URL `f`, generate reference with `.translate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ref(f):\n",
    "    return f, SingleHdf5ToZarr(\n",
    "        f, \n",
    "        inline_threshold=300, \n",
    "        storage_options={\n",
    "            'anon' : True\n",
    "        }\n",
    "    ).translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map `gen_ref` to each member of `flist_bag` and compute\n",
    "Dask bag is a way to map a function to a set of inputs. This next couple blocks of code tell Dask to take all the files in `flist`, break them up into the same amount of partitions and map each partition to the `gen_ref()` function -- essentially mapping each file path to `gen_ref()`. Calling `bag.compute()` on this runs `gen_ref()` in parallel with as many workers as are available in Dask client.\n",
    "\n",
    "_Note: if running interactively on Binder, this will take a while since only one worker is available and the references will have to be generated in serial. See option for loading from jsons below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bag = db.from_sequence(flist).map(gen_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dicts = bag.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each url in `flist` has been used to generate a dictionary of reference data in `dicts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _Save/load references to/from JSON files (optional)_\n",
    "The individual dictionaries can be saved as JSON files if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./jsons', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "for (url, d) in dicts:\n",
    "    # Generate name from corresponding URL:\n",
    "    # Grab URL, strip everything but the filename, \n",
    "    # and replace .nc with .json\n",
    "    json_name = url.split('/')[-1].replace('.nc','.json')\n",
    "    with open(f'./jsons/{json_name}', 'w') as outf:\n",
    "        outf.write(ujson.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generated jsons can then be loaded back in as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# dicts = []\n",
    "\n",
    "# for f in sorted(glob('./example_jsons/individual/*.json')):\n",
    "#     with open(f,'r') as fin:\n",
    "#         dicts.append(ujson.load(fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "references = [d[1] for d in dicts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Use `MultiZarrToZarr` to combine the 24 individual references into a single reference\n",
    "In this example we passed a list of reference dictionaries, but you can also give it a list of `.json` filepaths (commented out)\n",
    "\n",
    "_Note: the Kerchunk `MultiZarrToZarr` API changed between versions 0.0.5 and 0.0.6. This part assumes the latest version at this time (0.0.6). Please see https://fsspec.github.io/kerchunk/reference.html#kerchunk.combine.MultiZarrToZarr for more details_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    references,\n",
    "    # sorted((glob('./example_jsons/individual/*.json'))),\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    concat_dims='t',\n",
    "    inline_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References can be saved to a file (`combined.json`) or passed back as a dictionary (`mzz_dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mzz.translate('./combined.json')\n",
    "# mzz_dict = mzz.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d853cbf2f35f45a59f79ca5e397d8dd1594080251b0b51418fe33f5fb0138a7a"
  },
  "kernelspec": {
   "display_name": "global-global-pangeo-dev",
   "language": "python",
   "name": "conda-env-global-global-pangeo-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
